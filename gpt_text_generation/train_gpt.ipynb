{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"files/OldManAndSea.txt\",\"r\", encoding='utf-8-sig') as f:\n",
    "    text=f.read()\n",
    "# 加载原始文本并将其拆分为单个字符\n",
    "text=list(text)\n",
    "for i in range(len(text)):\n",
    "    if text[i]=='\"':\n",
    "        if text[i+1]==' ' or text[i+1]=='\\n':\n",
    "            # 如果直引号后跟一个空格或换行符，则将其更改为 ” 引号\n",
    "            text[i]='”'\n",
    "        if text[i+1]!=' ' and text[i+1]!='\\n':\n",
    "            # 否则，将其更改为 “ 引号\n",
    "            text[i]='“'    #C\n",
    "    if text[i]==\"'\":\n",
    "        if text[i-1]!=' ' and text[i-1]!='\\n':\n",
    "            # 将直单引号转换为 ’ 号\n",
    "            text[i]='’'\n",
    "# 将单个字符重新连接回文本\n",
    "text=\"\".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He was an old man who fished alone in a skiff in the Gulf Stream and he\n",
      "had gone eighty-four days now without taking a fish.  In the first\n",
      "forty days a boy had been with him.  But after forty days without a\n",
      "fish the boy’s parents had told him that th\n"
     ]
    }
   ],
   "source": [
    "# 从第二本小说读取文本\n",
    "with open(\"files/ToWhomTheBellTolls.txt\",\"r\", encoding='utf-8-sig') as f:\n",
    "    text1=f.read()\n",
    "# 从第三本小说读取文本\n",
    "with open(\"files/FarewellToArms.txt\",\"r\", encoding='utf-8-sig') as f:\n",
    "    text2=f.read()\n",
    "# 合并三本小说的文本\n",
    "text=text+\" \"+text1+\" \"+text2    #C\n",
    "\n",
    "with open(\"files/ThreeNovels.txt\",\"w\", \n",
    "          encoding='utf-8-sig') as f:\n",
    "    # 保存合并后的文本\n",
    "    f.write(text)\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‘', '?', ';', ' ', '“', '.', ':', '-', '’', '!', ')', '&', '(', '”', ',']\n",
      "10599\n"
     ]
    }
   ],
   "source": [
    "# 将换行符替换为空格\n",
    "text=text.lower().replace(\"\\n\", \" \")\n",
    "chars=set(text.lower())\n",
    "# 识别所有标点符号\n",
    "punctuations=[i for i in chars if i.isalpha()==False\n",
    "              and i.isdigit()==False]\n",
    "print(punctuations)\n",
    "\n",
    "for x in punctuations:\n",
    "    # 在标点符号周围插入空格\n",
    "    text=text.replace(f\"{x}\", f\" {x} \")\n",
    "text_tokenized=text.split()\n",
    "# 计算独特词元的数量\n",
    "unique_tokens=set(text_tokenized)\n",
    "print(len(unique_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the text contains 364463 words\n",
      "there are 10600 unique tokens\n",
      "{'.': 0, 'the': 1, ',': 2, '“': 3, '”': 4, 'and': 5, 'i': 6, 'he': 7, 'to': 8, 'it': 9}\n",
      "{0: '.', 1: 'the', 2: ',', 3: '“', 4: '”', 5: 'and', 6: 'i', 7: 'he', 8: 'to', 9: 'it'}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter   \n",
    "\n",
    "word_counts=Counter(text_tokenized)    \n",
    "words=sorted(word_counts, key=word_counts.get,\n",
    "                      reverse=True)     \n",
    "# 添加 UNK 词元\n",
    "words.append(\"UNK\")\n",
    "text_length=len(text_tokenized)\n",
    "# 计算词汇表的大小\n",
    "ntokens=len(words)\n",
    "print(f\"the text contains {text_length} words\")\n",
    "print(f\"there are {ntokens} unique tokens\")\n",
    "# 将词元映射到索引\n",
    "word_to_int={v:k for k,v in enumerate(words)}\n",
    "# 将索引映射到词元\n",
    "int_to_word={v:k for k,v in word_to_int.items()}\n",
    "print({k:v for k,v in word_to_int.items() if k in words[:10]})\n",
    "print({k:v for k,v in int_to_word.items() if v in words[:10]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'was', 'an', 'old', 'man', 'who', 'fished', 'alone', 'in', 'a', 'skiff', 'in', 'the', 'gulf', 'stream', 'and', 'he', 'had', 'gone', 'eighty']\n",
      "[7, 14, 99, 93, 63, 85, 3818, 311, 15, 11, 657, 15, 1, 2369, 514, 5, 7, 24, 220, 2016]\n"
     ]
    }
   ],
   "source": [
    "print(text_tokenized[0:20])\n",
    "wordidx=[word_to_int[w] for w in text_tokenized]  \n",
    "print([word_to_int[w] for w in text_tokenized[0:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# 将序列长度设置为128个索引\n",
    "seq_len=128  \n",
    "xys=[]\n",
    "for n in range(0, len(wordidx)-seq_len-1):\n",
    "    # 输入序列x包含训练文本中的128个连续索引\n",
    "    x = wordidx[n:n+seq_len]\n",
    "    # 将x向右移动一个位置，并将其作为输出y\n",
    "    y = wordidx[n+1:n+seq_len+1]\n",
    "    # 将(x, y)对添加到训练数据中\n",
    "    xys.append((torch.tensor(x),(torch.tensor(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  37,    0,   25,  ...,   21,  122,  103],\n",
      "        [9350,    1,  249,  ...,  336,  512,   53],\n",
      "        [1388,   34,  136,  ...,   51,   12, 1844],\n",
      "        ...,\n",
      "        [   5,  200,   19,  ...,  180,   18,    4],\n",
      "        [ 877,  199,    2,  ...,    4,    3,   71],\n",
      "        [  34, 1797,    0,  ..., 1617,  584,    7]])\n",
      "tensor([[   0,   25,  531,  ...,  122,  103,  575],\n",
      "        [   1,  249,    5,  ...,  512,   53,   52],\n",
      "        [  34,  136,  203,  ...,   12, 1844,   18],\n",
      "        ...,\n",
      "        [ 200,   19, 1757,  ...,   18,    4,   41],\n",
      "        [ 199,    2,    4,  ...,    3,   71,   87],\n",
      "        [1797,    0,    1,  ...,  584,    7, 1207]])\n",
      "torch.Size([32, 128]) torch.Size([32, 128])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size=32\n",
    "loader = DataLoader(xys, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "x,y=next(iter(loader))\n",
    "print(x)\n",
    "print(y)\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5*x*(1.0+torch.tanh(math.sqrt(2.0/math.pi)*\\\n",
    "                       (x + 0.044715 * torch.pow(x, 3.0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.n_layer = 3\n",
    "        self.n_head = 4\n",
    "        self.n_embd = 256\n",
    "        self.vocab_size = ntokens\n",
    "        self.block_size = 128 \n",
    "        self.embd_pdrop = 0.1\n",
    "        self.resid_pdrop = 0.1\n",
    "        self.attn_pdrop = 0.1\n",
    "        \n",
    "config=Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(\\\n",
    "                   config.block_size, config.block_size))\n",
    "             .view(1, 1, config.block_size, config.block_size))\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() \n",
    "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        hs = C // self.n_head\n",
    "        k = k.view(B, T, self.n_head, hs).transpose(1, 2) \n",
    "        q = q.view(B, T, self.n_head, hs).transpose(1, 2) \n",
    "        v = v.view(B, T, self.n_head, hs).transpose(1, 2) \n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) *\\\n",
    "            (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, \\\n",
    "                              float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = nn.ModuleDict(dict(\n",
    "            c_fc   = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            c_proj = nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            act    = GELU(),\n",
    "            dropout = nn.Dropout(config.resid_pdrop),\n",
    "        ))\n",
    "        m = self.mlp\n",
    "        self.mlpf=lambda x:m.dropout(m.c_proj(m.act(m.c_fc(x)))) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlpf(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.embd_pdrop),\n",
    "            h = nn.ModuleList([Block(config) \n",
    "                               for _ in range(config.n_layer)]),   \n",
    "            ln_f = nn.LayerNorm(config.n_embd),))\n",
    "        self.lm_head = nn.Linear(config.n_embd,\n",
    "                                 config.vocab_size, bias=False)      \n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):    \n",
    "                torch.nn.init.normal_(p, mean=0.0, \n",
    "                  std=0.02/math.sqrt(2 * config.n_layer))\n",
    "    def forward(self, idx, targets=None):\n",
    "        b, t = idx.size()\n",
    "        pos = torch.arange(0,t,dtype=torch.long).unsqueeze(0).to(device)\n",
    "        tok_emb = self.transformer.wte(idx) \n",
    "        pos_emb = self.transformer.wpe(pos) \n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 5.12M\n",
      "Model(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(10600, 256)\n",
      "    (wpe): Embedding(128, 256)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-2): 3 x Block(\n",
      "        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
      "          (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): ModuleDict(\n",
      "          (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (act): GELU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=256, out_features=10600, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model=Model(config)\n",
    "model.to(device)\n",
    "num=sum(p.numel() for p in model.transformer.parameters())\n",
    "print(\"number of parameters: %.2fM\" % (num/1e6,))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss 3.7760729258076786\n",
      "epoch 2 loss 2.676208010064096\n",
      "epoch 3 loss 2.127361443472855\n",
      "epoch 4 loss 1.7656190975592254\n",
      "epoch 5 loss 1.5094296120077073\n",
      "epoch 6 loss 1.3176188678852854\n",
      "epoch 7 loss 1.16998387734186\n",
      "epoch 8 loss 1.0534174176305664\n",
      "epoch 9 loss 0.9589971623844952\n",
      "epoch 10 loss 0.8814476710850045\n",
      "epoch 11 loss 0.8167824102331711\n",
      "epoch 12 loss 0.7622842388382993\n",
      "epoch 13 loss 0.7160491116767648\n",
      "epoch 14 loss 0.6771292377655271\n",
      "epoch 15 loss 0.6431691947530699\n",
      "epoch 16 loss 0.6140689558583321\n",
      "epoch 17 loss 0.5882501356236947\n",
      "epoch 18 loss 0.5654522757679512\n",
      "epoch 19 loss 0.5448244017199265\n",
      "epoch 20 loss 0.526271384592197\n",
      "epoch 21 loss 0.5096928720396348\n",
      "epoch 22 loss 0.4944293839759155\n",
      "epoch 23 loss 0.48043519630439246\n",
      "epoch 24 loss 0.46764937905902587\n",
      "epoch 25 loss 0.45589859904837526\n",
      "epoch 26 loss 0.4449365567589359\n",
      "epoch 27 loss 0.43470808733814154\n",
      "epoch 28 loss 0.4251141446390157\n",
      "epoch 29 loss 0.41644868460393386\n",
      "epoch 30 loss 0.4082125714096185\n",
      "epoch 31 loss 0.4000631030442822\n",
      "epoch 32 loss 0.39279487417072145\n",
      "epoch 33 loss 0.3859790013002089\n",
      "epoch 34 loss 0.3797579008425641\n",
      "epoch 35 loss 0.37342144606476946\n",
      "epoch 36 loss 0.36762030620305247\n",
      "epoch 37 loss 0.3623547640784688\n",
      "epoch 38 loss 0.35704594172619775\n",
      "epoch 39 loss 0.3518369748007952\n",
      "epoch 40 loss 0.34728748344922766\n"
     ]
    }
   ],
   "source": [
    "model.train()  \n",
    "for i in range(1,41):\n",
    "    tloss = 0.\n",
    "    # 遍历所有训练数据批次\n",
    "    for idx, (x,y) in enumerate(loader):\n",
    "        x,y=x.to(device),y.to(device)\n",
    "        output = model(x)\n",
    "        # 将模型预测与实际输出进行比较\n",
    "        loss=loss_func(output.view(-1,output.size(-1)),\n",
    "                           y.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # 将梯度范数裁剪为1\n",
    "        nn.utils.clip_grad_norm_(model.parameters(),1)\n",
    "        # 调整模型参数\n",
    "        optimizer.step()\n",
    "        tloss += loss.item()\n",
    "    print(f'epoch {i} loss {tloss/(idx+1)}') \n",
    "    if i%10==0:\n",
    "        # 每隔 10 个 epoch 保存一次模型\n",
    "        torch.save(model.state_dict(),f'files/GPTe{i}.pth') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(idx, weights, max_new_tokens, temperature=1.0, top_k=None):\n",
    "    model.eval()\n",
    "    # 加载训练过的模型\n",
    "    model.load_state_dict(torch.load(weights, weights_only=False))\n",
    "    original_length=len(idx[0])\n",
    "    # 生成固定数量的新索引\n",
    "    for _ in range(max_new_tokens):\n",
    "        if idx.size(1) <= config.block_size:\n",
    "            idx_cond = idx  \n",
    "        else:\n",
    "            idx_cond = idx[:, -config.block_size:]\n",
    "        # 使用模型进行预测\n",
    "        logits = model(idx_cond.to(device))\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        idx_next=torch.multinomial(probs,num_samples=1)\n",
    "        # 将新的索引添加到序列的末尾\n",
    "        idx = torch.cat((idx, idx_next.cpu()), dim=1)\n",
    "    # 仅输出新的索引\n",
    "    return idx[:, original_length:]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK=word_to_int[\"UNK\"]\n",
    "def generate(prompt, weights, max_new_tokens, temperature=1.0,\n",
    "             top_k=None):\n",
    "    # 确保提示并不为空\n",
    "    assert len(prompt)>0, \"prompt must contain at least one token\"\n",
    "    text=prompt.lower().replace(\"\\n\", \" \")\n",
    "    for x in punctuations:\n",
    "        text=text.replace(f\"{x}\", f\" {x} \")\n",
    "    text_tokenized=text.split() \n",
    "    # 将提示转换为索引序列\n",
    "    idx=[word_to_int.get(w,UNK) for w in text_tokenized]\n",
    "    idx=torch.LongTensor(idx).unsqueeze(0)\n",
    "    # 使用sample()函数生成新的索引\n",
    "    idx=sample(idx, weights, max_new_tokens, temperature=1.0, top_k=None)\n",
    "    # 将新的索引序列转换回文本\n",
    "    tokens=[int_to_word[i] for i in idx.squeeze().numpy()] \n",
    "    text=\" \".join(tokens)\n",
    "    for x in '''”).:;!?,-‘’''':\n",
    "        text=text.replace(f\" {x}\", f\"{x}\") \n",
    "    for x in '''“(-‘’''':\n",
    "        text=text.replace(f\"{x} \", f\"{x}\")     \n",
    "    return prompt+\" \"+text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shop. “all the poor hands and old fish,” he said. “but i was there\n",
      "--------------------------------------------------\n",
      ",” pablo said. “i do not like to have girls. i don’t care to\n",
      "--------------------------------------------------\n",
      ". it would be a great mistake to arrive at all regularly,” he said. “so long\n",
      "--------------------------------------------------\n",
      "completely and in the fall the cattle were cool. the lieutenant-colonel went in to be brought in\n",
      "--------------------------------------------------\n",
      "way. it was all right. this was what golz had talked about. the longer he was around\n",
      "--------------------------------------------------\n",
      "four o’clock in the morning at marseilles when the mistral was blowing. the major said he had\n",
      "--------------------------------------------------\n",
      "carefully through the woods at night. “do you know anything about being conquered and so you think it\n",
      "--------------------------------------------------\n",
      "chairs.” “in two places in this month the open town,” she said. “there\n",
      "--------------------------------------------------\n",
      "are in madrid with his slowness and brutality that is insupportable.” “in those who now are the\n",
      "--------------------------------------------------\n",
      "them packed their contents with the automatic rifle all through. three were heinkel one-elevens, twin-\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "prompt=\"UNK\"\n",
    "for i in range(10):\n",
    "    print(generate(prompt,'files/GPTe20.pth',max_new_tokens=20)[4:])\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?” “they’ll ring for us. we’ll have to be awfully careful. you’ll have to prove you.” “have another vermouth?” i asked. “no thank you. i have to make you any report if\n"
     ]
    }
   ],
   "source": [
    "prompt=\"UNK\"\n",
    "print(generate(prompt,'files/GPTe10.pth',max_new_tokens=50)[4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the old man saw the shark near the stars and his strange light the sun. it was cold after the sun went down and the old man\n",
      "--------------------------------------------------\n",
      "the old man saw the shark near the break through the water that was too far away as he knew. then he swung the tiller across the\n",
      "--------------------------------------------------\n",
      "the old man saw the shark near the stick break in the darkness; strange light the sun shone on him. he twisted the blade and as\n",
      "--------------------------------------------------\n",
      "the old man saw the shark near the stars. he must be an awfully old man and he will beat me. first he wore no water\n",
      "--------------------------------------------------\n",
      "the old man saw the shark near the break through the fish. he still hung to his left eye and then he put the oblong of the\n",
      "--------------------------------------------------\n",
      "the old man saw the shark near the stars. it was an easy shot now and he just then he got the mast on his shoulder and\n",
      "--------------------------------------------------\n",
      "the old man saw the shark near the break through the cloth. he saw a shark that was in the water as he drove down the shark\n",
      "--------------------------------------------------\n",
      "the old man saw the shark near the stars and checked his course. the line showed like a phosphorescent streak in the water straight out from his\n",
      "--------------------------------------------------\n",
      "the old man saw the shark near the other part that and slid down into the water. then he watched the shark come on. the old\n",
      "--------------------------------------------------\n",
      "the old man saw the shark near the stars and, driving at speed, when he was halfway to the far end of the bridge. it\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "prompt=\"the old man saw the shark near the\"\n",
    "for i in range(10):\n",
    "    print(generate(prompt,'files/GPTe40.pth',max_new_tokens=20))\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the old man saw the shark near the boy that. i must know that the sun was down and not let us go out and stay there\n",
      "--------------------------------------------------\n",
      "the old man saw the shark near the other lions. he still hung to a place where two stretcher-bearers stood beside fernando against the wall\n",
      "--------------------------------------------------\n",
      "the old man saw the shark near the gulf weed that were out and he loved in the water, now that the old man could see his\n",
      "--------------------------------------------------\n",
      "the old man saw the shark near the last almost unheard hum that comes like the rope to a club of aficionados banded together into the club finito\n",
      "--------------------------------------------------\n",
      "the old man saw the shark near the other boats. the old man’s head was clear and cold and his old legs. he was\n",
      "--------------------------------------------------\n",
      "the old man saw the shark near the same movement of the other three. he had taken his knife to go down to where he had placed\n",
      "--------------------------------------------------\n",
      "the old man saw the shark near the rope, wide, wide, dark haired, then a close to the surface he heard the first truck\n",
      "--------------------------------------------------\n",
      "the old man saw the shark near the fish, he thought. the old man let him hit the fish and then drove the knife on the\n",
      "--------------------------------------------------\n",
      "the old man saw the shark near the same bird pass circling himself and he did not wish to bring it to the fish. he stayed perfectly\n",
      "--------------------------------------------------\n",
      "the old man saw the shark near the rope to come around through the rocks where it is and. the old man will fly later to make\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "prompt=\"the old man saw the shark near the\"\n",
    "for i in range(10):\n",
    "    print(generate(prompt,'files/GPTe20.pth',max_new_tokens=20,\n",
    "                  temperature=0.9,top_k=50))\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the old man saw the shark near the stars. he leaned over the side and punched at him. he hit only meat and the hide was set hard and he barely got the knife in. the blow hurt not only his hands but his shoulder too. but the shark came up fast with his\n"
     ]
    }
   ],
   "source": [
    "prompt=\"the old man saw the shark near the\"\n",
    "print(generate(prompt,'files/GPTe40.pth',max_new_tokens=50,\n",
    "                  temperature=0.95,top_k=100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
